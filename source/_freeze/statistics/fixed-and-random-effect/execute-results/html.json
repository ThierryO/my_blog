{
  "hash": "3b2cccdcd4b3c1b61e38f82f5feb7180",
  "result": {
    "markdown": "---\ntitle: Using a variable both as fixed and random effect\nauthor: Thierry Onkelinx\ndate: \"2017-08-23\"\ncategories: [statistics, mixed-models]\nimage: fixed-and-random-effect_files/figure-html/fit-discrete-fit2-1.svg\nknitr:\n  opts_chunk: \n    echo: true\n    message: false\n    dev: \"svg\"\n---\n\n\nOne of the questions to answer when using mixed models is whether to use a variable as a fixed effect or as a random effect.\nSometimes it makes sense to use a variable both as fixed and random effect.\nIn this post I will try to make clear in which cases it can make sense and what are the benefits of doing so.\nI will also handle cases in which it doesn't make sense.\nMuch will depend on the nature of the variable.\nTherefore this post is split into three sections: categorical, discrete and continuous.\nI will only display the most relevant parts of the code.\nThe full code is available on [GitHub](https:/github.com/thierryo/my_blog).\n\n# Categorical variable\n\nTo make this clear, we start by creating a dummy dataset with 3 categorical covariates.\n`B` is nested within `A`.\nThe resulting dataset is displayed in @fig-cat-dummy.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nset.seed(20170823)\nn_a <- 6\nn_b <- 2\nn_sample <- 3\nsd_a <- 2\nsd_b <- 1\nsd_noise <- 1\ndataset <- expand.grid(\n  B = paste0(\"b\", seq_len(n_a * n_b)),\n  sample = seq_len(n_sample)\n) %>%\n  mutate(\n    A = paste0(\"a\", as.integer(B) %% n_a) %>%\n      factor(),\n    mu = rnorm(n_a, sd = sd_a)[A] + rnorm(n_a * n_b, sd = sd_b)[B],\n    Y = mu + rnorm(n(), sd = sd_noise)\n  )\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![Dummy dataset with categorical variables.](fixed-and-random-effect_files/figure-html/fig-cat-dummy-1.svg){#fig-cat-dummy width=672}\n:::\n:::\n\n\nThe first model is one that doesn't make sense.\nUsing a categorical variable both as random and a fixed effect.\nIn this case both effects are competing for the same information.\nBelow is the resulting fit from `lme4` and `INLA`.\nNote the warning in the `lme4` output, the model failed to converge.\nNevertheless, both `lme4` and `INLA` yield the same parameter estimate (@fig-cat-fixed), albeit the much wider confidence intervals for `lme4`.\nThe estimates for the random effects in both packages are equivalent to zero (@fig-cat-random).\nAgain the `lme4` estimate has more uncertainty.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(lme4)\nmodel_1 <- lmer(Y ~ 0 + A + (1 | A), data = dataset)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv, :\nunable to evaluate scaled gradient\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv, :\nModel failed to converge: degenerate Hessian with 1 negative eigenvalues\n```\n:::\n\n```{.r .cell-code}\nsummary(model_1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear mixed model fit by REML ['lmerMod']\nFormula: Y ~ 0 + A + (1 | A)\n   Data: dataset\n\nREML criterion at convergence: 109.5\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-1.5304 -0.7336 -0.0650  0.5862  2.2205 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n A        (Intercept) 6.651    2.579   \n Residual             1.576    1.255   \nNumber of obs: 36, groups:  A, 6\n\nFixed effects:\n    Estimate Std. Error t value\nAa0  -4.0282     2.6294  -1.532\nAa1   1.1165     2.6294   0.425\nAa2  -1.2266     2.6294  -0.466\nAa3   2.6855     2.6294   1.021\nAa4   0.4843     2.6294   0.184\nAa5  -2.9922     2.6294  -1.138\n\nCorrelation of Fixed Effects:\n    Aa0   Aa1   Aa2   Aa3   Aa4  \nAa1 0.000                        \nAa2 0.000 0.000                  \nAa3 0.000 0.000 0.000            \nAa4 0.000 0.000 0.000 0.000      \nAa5 0.000 0.000 0.000 0.000 0.000\noptimizer (nloptwrap) convergence code: 0 (OK)\nunable to evaluate scaled gradient\nModel failed to converge: degenerate  Hessian with 1 negative eigenvalues\n```\n:::\n\n```{.r .cell-code}\nlibrary(INLA)\nmodel_2 <- inla(Y ~ 0 + A + f(A, model = \"iid\"), data = dataset)\nsummary(model_2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\n   c(\"inla.core(formula = formula, family = family, contrasts = contrasts, \n   \", \" data = data, quantiles = quantiles, E = E, offset = offset, \", \" \n   scale = scale, weights = weights, Ntrials = Ntrials, strata = strata, \n   \", \" lp.scale = lp.scale, link.covariates = link.covariates, verbose = \n   verbose, \", \" lincomb = lincomb, selection = selection, control.compute \n   = control.compute, \", \" control.predictor = control.predictor, \n   control.family = control.family, \", \" control.inla = control.inla, \n   control.fixed = control.fixed, \", \" control.mode = control.mode, \n   control.expert = control.expert, \", \" control.hazard = control.hazard, \n   control.lincomb = control.lincomb, \", \" control.update = \n   control.update, control.lp.scale = control.lp.scale, \", \" \n   control.pardiso = control.pardiso, only.hyperparam = only.hyperparam, \n   \", \" inla.call = inla.call, inla.arg = inla.arg, num.threads = \n   num.threads, \", \" keep = keep, working.directory = working.directory, \n   silent = silent, \", \" inla.mode = inla.mode, safe = FALSE, debug = \n   debug, .parent.frame = .parent.frame)\" ) \nTime used:\n    Pre = 0.992, Running = 0.838, Post = 0.0399, Total = 1.87 \nFixed effects:\n      mean    sd 0.025quant 0.5quant 0.975quant   mode kld\nAa0 -4.027 0.508     -5.029   -4.027     -3.025 -4.027   0\nAa1  1.116 0.508      0.114    1.116      2.118  1.116   0\nAa2 -1.226 0.508     -2.228   -1.226     -0.224 -1.226   0\nAa3  2.685 0.508      1.683    2.685      3.687  2.685   0\nAa4  0.484 0.508     -0.518    0.484      1.486  0.484   0\nAa5 -2.991 0.508     -3.993   -2.991     -1.989 -2.991   0\n\nRandom effects:\n  Name\t  Model\n    A IID model\n\nModel hyperparameters:\n                                            mean       sd 0.025quant 0.5quant\nPrecision for the Gaussian observations 6.82e-01 1.71e-01      0.399 6.64e-01\nPrecision for A                         2.17e+04 2.31e+04   1582.690 1.45e+04\n                                        0.975quant     mode\nPrecision for the Gaussian observations       1.07    0.631\nPrecision for A                           83100.47 4388.898\n\nMarginal log-Likelihood:  -91.89 \n is computed \nPosterior summaries for the linear predictor and the fitted values are computed\n(Posterior marginals needs also 'control.compute=list(return.marginals.predictor=TRUE)')\n```\n:::\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![Comparison of fixed effects parameters for model `A + (1|A)`](fixed-and-random-effect_files/figure-html/fig-cat-fixed-1.svg){#fig-cat-fixed width=672}\n:::\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![Comparison of random effects parameters for model `A + (1|A)`](fixed-and-random-effect_files/figure-html/fig-cat-random-1.svg){#fig-cat-random width=672}\n:::\n:::\n\n\nWhat if we want to add variable `B` as a nested random effect? We already know that adding `A` to both the fixed and the random effects is nonsense.\nThe correct way of doing this is to use `A` as a fixed effect and `B` as an [implicit nested](index.html) random effect.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_1 <- lmer(Y ~ 0 + A + (1 | A / B), data = dataset)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv, :\nunable to evaluate scaled gradient\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv, :\nModel failed to converge: degenerate Hessian with 1 negative eigenvalues\n```\n:::\n\n```{.r .cell-code}\nsummary(model_1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear mixed model fit by REML ['lmerMod']\nFormula: Y ~ 0 + A + (1 | A/B)\n   Data: dataset\n\nREML criterion at convergence: 105.6\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-1.4004 -0.4931 -0.1464  0.4092  2.1372 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n B:A      (Intercept) 0.7957   0.892   \n A        (Intercept) 1.8676   1.367   \n Residual             1.0983   1.048   \nNumber of obs: 36, groups:  B:A, 12; A, 6\n\nFixed effects:\n    Estimate Std. Error t value\nAa0  -4.0282     1.5648  -2.574\nAa1   1.1165     1.5648   0.714\nAa2  -1.2266     1.5648  -0.784\nAa3   2.6855     1.5648   1.716\nAa4   0.4843     1.5648   0.310\nAa5  -2.9922     1.5648  -1.912\n\nCorrelation of Fixed Effects:\n    Aa0   Aa1   Aa2   Aa3   Aa4  \nAa1 0.000                        \nAa2 0.000 0.000                  \nAa3 0.000 0.000 0.000            \nAa4 0.000 0.000 0.000 0.000      \nAa5 0.000 0.000 0.000 0.000 0.000\noptimizer (nloptwrap) convergence code: 0 (OK)\nunable to evaluate scaled gradient\nModel failed to converge: degenerate  Hessian with 1 negative eigenvalues\n```\n:::\n\n```{.r .cell-code}\nmodel_1b <- lmer(Y ~ 0 + A + (1 | B), data = dataset)\nsummary(model_1b)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear mixed model fit by REML ['lmerMod']\nFormula: Y ~ 0 + A + (1 | B)\n   Data: dataset\n\nREML criterion at convergence: 105.6\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-1.4004 -0.4931 -0.1464  0.4092  2.1372 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n B        (Intercept) 0.7957   0.892   \n Residual             1.0983   1.048   \nNumber of obs: 36, groups:  B, 12\n\nFixed effects:\n    Estimate Std. Error t value\nAa0  -4.0282     0.7622  -5.285\nAa1   1.1165     0.7622   1.465\nAa2  -1.2266     0.7622  -1.609\nAa3   2.6855     0.7622   3.523\nAa4   0.4843     0.7622   0.635\nAa5  -2.9922     0.7622  -3.926\n\nCorrelation of Fixed Effects:\n    Aa0   Aa1   Aa2   Aa3   Aa4  \nAa1 0.000                        \nAa2 0.000 0.000                  \nAa3 0.000 0.000 0.000            \nAa4 0.000 0.000 0.000 0.000      \nAa5 0.000 0.000 0.000 0.000 0.000\n```\n:::\n\n```{.r .cell-code}\nmodel_2 <- inla(\n  Y ~ 0 + A + f(A, model = \"iid\") + f(B, model = \"iid\"), data = dataset\n)\nsummary(model_2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\n   c(\"inla.core(formula = formula, family = family, contrasts = contrasts, \n   \", \" data = data, quantiles = quantiles, E = E, offset = offset, \", \" \n   scale = scale, weights = weights, Ntrials = Ntrials, strata = strata, \n   \", \" lp.scale = lp.scale, link.covariates = link.covariates, verbose = \n   verbose, \", \" lincomb = lincomb, selection = selection, control.compute \n   = control.compute, \", \" control.predictor = control.predictor, \n   control.family = control.family, \", \" control.inla = control.inla, \n   control.fixed = control.fixed, \", \" control.mode = control.mode, \n   control.expert = control.expert, \", \" control.hazard = control.hazard, \n   control.lincomb = control.lincomb, \", \" control.update = \n   control.update, control.lp.scale = control.lp.scale, \", \" \n   control.pardiso = control.pardiso, only.hyperparam = only.hyperparam, \n   \", \" inla.call = inla.call, inla.arg = inla.arg, num.threads = \n   num.threads, \", \" keep = keep, working.directory = working.directory, \n   silent = silent, \", \" inla.mode = inla.mode, safe = FALSE, debug = \n   debug, .parent.frame = .parent.frame)\" ) \nTime used:\n    Pre = 0.631, Running = 0.864, Post = 0.0183, Total = 1.51 \nFixed effects:\n      mean    sd 0.025quant 0.5quant 0.975quant   mode kld\nAa0 -4.027 0.586     -5.183   -4.027     -2.870 -4.027   0\nAa1  1.116 0.586     -0.040    1.116      2.273  1.116   0\nAa2 -1.226 0.586     -2.383   -1.226     -0.070 -1.226   0\nAa3  2.685 0.586      1.528    2.685      3.841  2.685   0\nAa4  0.484 0.586     -0.672    0.484      1.641  0.484   0\nAa5 -2.991 0.586     -4.147   -2.991     -1.835 -2.991   0\n\nRandom effects:\n  Name\t  Model\n    A IID model\n   B IID model\n\nModel hyperparameters:\n                                            mean       sd 0.025quant 0.5quant\nPrecision for the Gaussian observations 8.52e-01 2.58e-01      0.414 8.25e-01\nPrecision for A                         2.17e+04 2.31e+04   1581.172 1.46e+04\nPrecision for B                         1.81e+01 5.96e+01      0.649 5.80e+00\n                                        0.975quant     mode\nPrecision for the Gaussian observations       1.41    0.789\nPrecision for A                           83018.44 4384.322\nPrecision for B                             116.79    1.484\n\nMarginal log-Likelihood:  -97.31 \n is computed \nPosterior summaries for the linear predictor and the fitted values are computed\n(Posterior marginals needs also 'control.compute=list(return.marginals.predictor=TRUE)')\n```\n:::\n\n```{.r .cell-code}\nmodel_2b <- inla(Y ~ 0 + A + f(B, model = \"iid\"), data = dataset)\nsummary(model_2b)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\n   c(\"inla.core(formula = formula, family = family, contrasts = contrasts, \n   \", \" data = data, quantiles = quantiles, E = E, offset = offset, \", \" \n   scale = scale, weights = weights, Ntrials = Ntrials, strata = strata, \n   \", \" lp.scale = lp.scale, link.covariates = link.covariates, verbose = \n   verbose, \", \" lincomb = lincomb, selection = selection, control.compute \n   = control.compute, \", \" control.predictor = control.predictor, \n   control.family = control.family, \", \" control.inla = control.inla, \n   control.fixed = control.fixed, \", \" control.mode = control.mode, \n   control.expert = control.expert, \", \" control.hazard = control.hazard, \n   control.lincomb = control.lincomb, \", \" control.update = \n   control.update, control.lp.scale = control.lp.scale, \", \" \n   control.pardiso = control.pardiso, only.hyperparam = only.hyperparam, \n   \", \" inla.call = inla.call, inla.arg = inla.arg, num.threads = \n   num.threads, \", \" keep = keep, working.directory = working.directory, \n   silent = silent, \", \" inla.mode = inla.mode, safe = FALSE, debug = \n   debug, .parent.frame = .parent.frame)\" ) \nTime used:\n    Pre = 0.52, Running = 0.863, Post = 0.016, Total = 1.4 \nFixed effects:\n      mean    sd 0.025quant 0.5quant 0.975quant   mode kld\nAa0 -4.027 0.625     -5.261   -4.027     -2.792 -4.027   0\nAa1  1.116 0.625     -0.119    1.116      2.351  1.116   0\nAa2 -1.226 0.625     -2.461   -1.226      0.009 -1.226   0\nAa3  2.684 0.625      1.450    2.684      3.919  2.684   0\nAa4  0.484 0.625     -0.751    0.484      1.719  0.484   0\nAa5 -2.991 0.625     -4.226   -2.991     -1.756 -2.991   0\n\nRandom effects:\n  Name\t  Model\n    B IID model\n\nModel hyperparameters:\n                                          mean   sd 0.025quant 0.5quant\nPrecision for the Gaussian observations 453.14 0.00     216.14   453.14\nPrecision for B                           0.00 0.00       0.00     0.00\n                                        0.975quant   mode\nPrecision for the Gaussian observations    7261.85 453.14\nPrecision for B                               0.00   0.00\n\nMarginal log-Likelihood:  -94.33 \n is computed \nPosterior summaries for the linear predictor and the fitted values are computed\n(Posterior marginals needs also 'control.compute=list(return.marginals.predictor=TRUE)')\n```\n:::\n:::\n\n\n# Discrete variable\n\n## Intro\n\nA discrete variable is a numerical variable but each interval between two values is an integer multiple of a fixed step size.\nTypical examples are related to time, e.g. the year in steps of 1 year, months expressed in terms of years (step size 1/12), ...\n\nWe create a new dummy dataset with a discrete variable.\nThe response variable is a third order polynomial of the discrete variable.\nThe `X` variable is rescaled to -1 and 1.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn_x <- 25\nn_sample <- 10\nsd_noise <- 10\ndataset <- expand.grid(\n  X = seq_len(n_x),\n  sample = seq_len(n_sample)\n) %>%\n  mutate(\n    mu =  0.045 * X ^ 3 - X ^ 2 + 10,\n    Y = mu + rnorm(n(), sd = sd_noise),\n    X = (X - ceiling(n_x / 2)) / floor(n_x / 2)\n  )\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![Dummy dataset with a discrete variable. The line represents the true model.](fixed-and-random-effect_files/figure-html/fig-discrete-dummy-1.svg){#fig-discrete-dummy width=672}\n:::\n:::\n\n\n## Fit with `lme4`\n\nSuppose we fit a simple linear model to the data.\nWe know that this is not accurate because the real pattern is a third order polynomial.\nAnd let's add the variable also as a random effect.\nWe use first `lme4` to illustrate the principle.\n(@fit-discrete-fit) illustrate how the fit of the fixed part is poor but the random effect of X compensates the fit.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_1 <- lmer(Y ~ X + (1 | X), data = dataset)\nsummary(model_1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear mixed model fit by REML ['lmerMod']\nFormula: Y ~ X + (1 | X)\n   Data: dataset\n\nREML criterion at convergence: 1943.1\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.24578 -0.67660 -0.02946  0.60728  3.00837 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n X        (Intercept) 1442.26  37.977  \n Residual               88.59   9.412  \nNumber of obs: 250, groups:  X, 25\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)  -20.886      7.619  -2.741\nX             11.388     12.678   0.898\n\nCorrelation of Fixed Effects:\n  (Intr)\nX 0.000 \n```\n:::\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![Fitted values (line) and observed values (points) from the lme4 model.](fixed-and-random-effect_files/figure-html/fit-discrete-fit-1.svg){width=672}\n:::\n:::\n\n\nThe overall model fit improves when we add a second and third polynomial term.\nAnd the variance of the random effect decreases.\nIt reduces even to zero once the third polynomial is in the model.\n(@fit-discrete-fit2) illustrates how the fit of the fixed effect improves when adding the higher order terms.\nThe effect on the fitted values with the random effect is marginal.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_1b <- lmer(Y ~ X + I(X ^ 2) + (1 | X), data = dataset)\nmodel_1c <- lmer(Y ~ X + I(X ^ 2) + I(X ^ 3) + (1 | X), data = dataset)\nanova(model_1, model_1b, model_1c)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nData: dataset\nModels:\nmodel_1: Y ~ X + (1 | X)\nmodel_1b: Y ~ X + I(X^2) + (1 | X)\nmodel_1c: Y ~ X + I(X^2) + I(X^3) + (1 | X)\n         npar    AIC    BIC  logLik deviance  Chisq Df Pr(>Chisq)    \nmodel_1     4 1963.9 1978.0 -977.93   1955.9                         \nmodel_1b    5 1914.3 1931.9 -952.15   1904.3 51.555  1  6.961e-13 ***\nmodel_1c    6 1836.9 1858.0 -912.46   1824.9 79.387  1  < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n\n```{.r .cell-code}\nsummary(model_1b)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear mixed model fit by REML ['lmerMod']\nFormula: Y ~ X + I(X^2) + (1 | X)\n   Data: dataset\n\nREML criterion at convergence: 1889.6\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.28558 -0.63406 -0.03279  0.61428  3.04912 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n X        (Intercept) 184.07   13.567  \n Residual              88.59    9.412  \nNumber of obs: 250, groups:  X, 25\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)  -59.143      4.173 -14.174\nX             11.388      4.623   2.463\nI(X^2)       105.943      8.622  12.288\n\nCorrelation of Fixed Effects:\n       (Intr) X     \nX       0.000       \nI(X^2) -0.746  0.000\n```\n:::\n\n```{.r .cell-code}\nsummary(model_1c)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear mixed model fit by REML ['lmerMod']\nFormula: Y ~ X + I(X^2) + I(X^3) + (1 | X)\n   Data: dataset\n\nREML criterion at convergence: 1814.9\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.4324 -0.6651  0.0087  0.6556  3.4151 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n X        (Intercept)  0.00    0.000   \n Residual             88.05    9.384   \nNumber of obs: 250, groups:  X, 25\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept) -59.1432     0.8914  -66.35\nX           -37.6013     2.4830  -15.14\nI(X^2)      105.9426     1.8419   57.52\nI(X^3)       75.5290     3.5123   21.50\n\nCorrelation of Fixed Effects:\n       (Intr) X      I(X^2)\nX       0.000              \nI(X^2) -0.746  0.000       \nI(X^3)  0.000 -0.917  0.000\noptimizer (nloptwrap) convergence code: 0 (OK)\nboundary (singular) fit: see help('isSingular')\n```\n:::\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![Fitted values from the fixed and random part of the `lme4` models. Points represent the true model.](fixed-and-random-effect_files/figure-html/fit-discrete-fit2-1.svg){width=672}\n:::\n:::\n\n\n## Fit with `INLA`\n\n`INLA` requires that we alter the data to get the same output.\nFirst we copy `X` into `X.copy` because `inla` allows the variable to be used only once in the formula.\nFor some reason this wasn't needed with the categorical variables.\nThe `lme4` syntax `X + (1|X)` translates into the following `INLA` syntax: `X + f(X.copy, model = \"iid\")`.\nThen next thing is that `INLA` does the model fitting and prediction in a single step.\nGetting predictions for new data requires to add the new data to the original data while setting the response to `NA`.\nIf we want predictions for the fixed effect only, then we need to add rows were all random effect covariates are set to `NA`.\nHence `X.copy` must be `NA` while `X` must be non `NA`.\nNote that this would be impossible without creating `X.copy`.\n\nLet's fit the three same models with `INLA`.\nThe predictions are given in @fig-discrete-fit3.\nThe results are very similar to the `lme4` results.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndataset2 <- dataset %>%\n  mutate(X.copy = X) %>%\n  bind_rows(\n    dataset %>%\n      distinct(X, mu)\n  )\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_2 <- inla(\n  Y ~ X + f(X.copy, model = \"iid\"), data = dataset2,\n  control.compute = list(waic = TRUE)\n)\nmodel_2b <- inla(\n  Y ~ X + I(X ^ 2) + f(X.copy, model = \"iid\"), data = dataset2,\n  control.compute = list(waic = TRUE)\n)\nmodel_2c <- inla(\n  Y ~ X + I(X ^ 2) + I(X ^ 3) + f(X.copy, model = \"iid\"), data = dataset2,\n  control.compute = list(waic = TRUE)\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n *** inla.core.safe:  rerun to try to solve negative eigenvalue(s) in the Hessian \n```\n:::\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![Fitted values from the fixed and random part of the `INLA` models. Points represent the true model.](fixed-and-random-effect_files/figure-html/fig-discrete-fit3-1.svg){#fig-discrete-fit3 width=672}\n:::\n:::\n\n\n# Continuous variable\n\nA continuous variable is a numeric variable where there is not fixed step size between two values.\nIn practice the step size will be several magnitudes smaller than the measured values.\nAgain let's clarify this with an example dataset.\nFor the sake of simplicity we'll reuse the true model from the example with the discrete variable.\nCompare @fig-continuous-dummy with @fig-discrete-dummy and you'll see that @fig-continuous-dummy has no step size while @fig-discrete-dummy does.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn_x <- 25\nn_sample <- 10\nsd_noise <- 10\ndataset <- data.frame(\n  X = runif(n_x * n_sample, min = 1, max = n_x)\n) %>%\n  mutate(\n    mu =  0.045 * X ^ 3 - X ^ 2 + 10,\n    Y = mu + rnorm(n(), sd = sd_noise),\n    X = (X - ceiling(n_x / 2)) / floor(n_x / 2),\n    X.copy = X\n  )\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![Dummy dataset with a continuous variable. The line represents the true model.](fixed-and-random-effect_files/figure-html/fig-continuous-dummy-1.svg){#fig-continuous-dummy width=672}\n:::\n:::\n\n\nThe `lmer` model fails because the random effect has as many unique values as observations.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntryCatch(\n  lmer(Y ~ X + (1 | X), data = dataset), error = identity\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<simpleError: number of levels of each grouping factor must be < number of observations (problems: X)>\n```\n:::\n\n```{.r .cell-code}\nlmer(Y ~ X + (1 | X), data = dataset)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError: number of levels of each grouping factor must be < number of observations (problems: X)\n```\n:::\n:::\n\n\nThe `INLA` model yields output but the variance of the random effect is very high.\nA good indicator that there is something wrong.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_2 <- inla(\n  Y ~ X + f(X.copy, model = \"iid\"), data = dataset,\n  control.compute = list(waic = TRUE)\n)\ninla.tmarginal(\n  function(x) {\n    x ^ -1\n  },\n  model_2$marginals.hyperpar$`Precision for X.copy`\n) %>%\n  inla.zmarginal()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMean            0.00011261 \nStdev           0.000163188 \nQuantile  0.025 8.88423e-06 \nQuantile  0.25  3.16364e-05 \nQuantile  0.5   6.21869e-05 \nQuantile  0.75  0.000127047 \nQuantile  0.975 0.000518334 \n```\n:::\n:::\n\n\n## Conclusion\n\nUsing a variable both in the fixed and random part of the model makes only sense in case of a discrete variable.\n\n## Session info\n\nThese R packages were used to create this post.\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.1 (2023-06-16)\n os       Ubuntu 22.04.3 LTS\n system   x86_64, linux-gnu\n ui       X11\n language nl_BE:nl\n collate  nl_BE.UTF-8\n ctype    nl_BE.UTF-8\n tz       Europe/Brussels\n date     2023-08-30\n pandoc   3.1.1 @ /usr/lib/rstudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package      * version    date (UTC) lib source\n boot           1.3-28.1   2022-11-22 [1] CRAN (R 4.3.0)\n class          7.3-22     2023-05-03 [4] CRAN (R 4.3.1)\n classInt       0.4-9      2023-02-28 [1] CRAN (R 4.3.1)\n cli            3.6.1      2023-03-23 [1] CRAN (R 4.3.0)\n colorspace     2.1-0      2023-01-23 [1] CRAN (R 4.3.0)\n DBI            1.1.3      2022-06-18 [1] CRAN (R 4.3.0)\n digest         0.6.32     2023-06-26 [1] CRAN (R 4.3.1)\n dplyr        * 1.1.2      2023-04-20 [1] CRAN (R 4.3.0)\n e1071          1.7-13     2023-02-01 [1] CRAN (R 4.3.1)\n evaluate       0.21       2023-05-05 [1] CRAN (R 4.3.0)\n fansi          1.0.4      2023-01-22 [1] CRAN (R 4.3.0)\n farver         2.1.1      2022-07-06 [1] CRAN (R 4.3.0)\n fastmap        1.1.1      2023-02-24 [1] CRAN (R 4.3.0)\n forcats      * 1.0.0      2023-01-29 [1] CRAN (R 4.3.0)\n generics       0.1.3      2022-07-05 [1] CRAN (R 4.3.0)\n ggplot2      * 3.4.2      2023-04-03 [1] CRAN (R 4.3.0)\n glue           1.6.2      2022-02-24 [1] CRAN (R 4.3.0)\n gtable         0.3.3      2023-03-21 [1] CRAN (R 4.3.0)\n hms            1.1.3      2023-03-21 [1] CRAN (R 4.3.0)\n htmltools      0.5.5      2023-03-23 [1] CRAN (R 4.3.0)\n htmlwidgets    1.6.2      2023-03-17 [1] CRAN (R 4.3.0)\n INLA         * 23.06.29   2023-06-30 [1] local\n inlabru        2.8.0      2023-06-20 [1] CRAN (R 4.3.1)\n jsonlite       1.8.7      2023-06-29 [1] CRAN (R 4.3.1)\n KernSmooth     2.23-21    2023-05-03 [1] CRAN (R 4.3.0)\n knitr          1.43       2023-05-25 [1] CRAN (R 4.3.0)\n labeling       0.4.2      2020-10-20 [1] CRAN (R 4.3.0)\n lattice        0.21-8     2023-04-05 [4] CRAN (R 4.3.0)\n lifecycle      1.0.3      2022-10-07 [1] CRAN (R 4.3.0)\n lme4         * 1.1-34     2023-07-04 [1] CRAN (R 4.3.1)\n lubridate    * 1.9.2.9000 2023-05-15 [1] https://inbo.r-universe.dev (R 4.3.0)\n magrittr       2.0.3      2022-03-30 [1] CRAN (R 4.3.0)\n MASS           7.3-60     2023-05-04 [4] CRAN (R 4.3.1)\n Matrix       * 1.5-4.1    2023-05-18 [1] CRAN (R 4.3.0)\n MatrixModels   0.5-1      2022-09-11 [1] CRAN (R 4.3.0)\n minqa          1.2.5      2022-10-19 [1] CRAN (R 4.3.0)\n munsell        0.5.0      2018-06-12 [1] CRAN (R 4.3.0)\n nlme           3.1-162    2023-01-31 [1] CRAN (R 4.3.0)\n nloptr         2.0.3      2022-05-26 [1] CRAN (R 4.3.0)\n numDeriv       2016.8-1.1 2019-06-06 [1] CRAN (R 4.3.0)\n pillar         1.9.0      2023-03-22 [1] CRAN (R 4.3.0)\n pkgconfig      2.0.3      2019-09-22 [1] CRAN (R 4.3.0)\n proxy          0.4-27     2022-06-09 [1] CRAN (R 4.3.1)\n purrr        * 1.0.1      2023-01-10 [1] CRAN (R 4.3.0)\n R6             2.5.1      2021-08-19 [1] CRAN (R 4.3.0)\n Rcpp           1.0.10     2023-01-22 [1] CRAN (R 4.3.0)\n readr        * 2.1.4      2023-02-10 [1] CRAN (R 4.3.0)\n rlang          1.1.1      2023-04-28 [1] CRAN (R 4.3.0)\n rmarkdown      2.23       2023-07-01 [1] CRAN (R 4.3.1)\n rstudioapi     0.14       2022-08-22 [1] CRAN (R 4.3.0)\n scales         1.2.1      2022-08-20 [1] CRAN (R 4.3.0)\n sessioninfo    1.2.2      2021-12-06 [1] CRAN (R 4.3.0)\n sf             1.0-13     2023-05-24 [1] CRAN (R 4.3.0)\n sp           * 2.0-0      2023-06-22 [1] CRAN (R 4.3.1)\n stringi        1.7.12     2023-01-11 [1] CRAN (R 4.3.0)\n stringr      * 1.5.0      2022-12-02 [1] CRAN (R 4.3.0)\n tibble       * 3.2.1      2023-03-20 [1] CRAN (R 4.3.0)\n tidyr        * 1.3.0      2023-01-24 [1] CRAN (R 4.3.0)\n tidyselect     1.2.0      2022-10-10 [1] CRAN (R 4.3.0)\n tidyverse    * 2.0.0      2023-02-22 [1] CRAN (R 4.3.0)\n timechange     0.2.0      2023-01-11 [1] CRAN (R 4.3.0)\n tzdb           0.4.0      2023-05-12 [1] CRAN (R 4.3.0)\n units          0.8-2      2023-04-27 [1] CRAN (R 4.3.0)\n utf8           1.2.3      2023-01-31 [1] CRAN (R 4.3.0)\n vctrs          0.6.3      2023-06-14 [1] CRAN (R 4.3.0)\n withr          2.5.0      2022-03-03 [1] CRAN (R 4.3.0)\n xfun           0.39       2023-04-20 [1] CRAN (R 4.3.0)\n yaml           2.3.7      2023-01-23 [1] CRAN (R 4.3.0)\n\n [1] /home/thierry/R/x86_64-pc-linux-gnu-library/4.3\n [2] /usr/local/lib/R/site-library\n [3] /usr/lib/R/site-library\n [4] /usr/lib/R/library\n\n──────────────────────────────────────────────────────────────────────────────\n```\n:::\n:::\n",
    "supporting": [
      "fixed-and-random-effect_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}