{
  "hash": "81b19c02d04e63eb9a1b83f6227d7f72",
  "result": {
    "markdown": "---\ntitle: Highly correlated random effects\nauthor: Thierry Onkelinx\ndate: \"2018-02-16\"\ncategories: [statistics, mixed-models]\nimage: highly-correlated_files/figure-html/fig-scatter-is-1.png\n---\n\n\nRecently, I got a question on a mixed model with highly correlated random slopes.\nI requested a copy of the data because it is much easier to diagnose the problem when you have the actual data.\nThe data owner gave permission to use an anonymised version of the data for this blog post.\nIn this blog post, I will discuss how I'd tackle this problem.\n\n# Data exploration\n\nEvery data analysis should start with some data exploration.\nThe dataset contains three variables: the response $Y$, a covariate $X$ and a grouping variable $ID$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(lme4)\ndataset <- read_csv(\"data/highly-correlated.csv\")\nsummary(dataset)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       Y                X               ID       \n Min.   : 40.05   Min.   :1.054   Min.   :  1.0  \n 1st Qu.: 72.25   1st Qu.:1.397   1st Qu.:148.0  \n Median : 88.99   Median :1.563   Median :273.0  \n Mean   : 99.34   Mean   :1.669   Mean   :286.9  \n 3rd Qu.:115.92   3rd Qu.:1.836   3rd Qu.:434.0  \n Max.   :351.49   Max.   :4.220   Max.   :601.0  \n```\n:::\n:::\n\n\nLet's start by looking at a scatter plot (@fig-scatter).\nThis suggests a strong linear relation between $X$ and $Y$.\nPlotting the point with transparency reveals that the density of the observations depends on the value.\nThis is confirmed by the skewed distribution shown in @fig-density.\nThis is something we have to keep in mind.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(dataset, aes(x = X, y = Y)) + geom_point(alpha = 0.1)\n```\n\n::: {.cell-output-display}\n![Scattterplot](highly-correlated_files/figure-html/fig-scatter-1.png){#fig-scatter width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndataset %>%\n  select(-ID) %>%\n  gather(\"Variable\", \"Value\") %>%\n  ggplot(aes(x = Value)) + geom_density() + facet_wrap(~Variable, scales = \"free\")\n```\n\n::: {.cell-output-display}\n![Density of the variables](highly-correlated_files/figure-html/fig-density-1.svg){#fig-density width=672}\n:::\n:::\n\n\nThe mathematical equation of the random slope model is given in @eq-original.\nIt contains a fixed intercept and fixed slope along $X$ and a random intercept and random slope along $X$ for each $ID$.\nThe random intercept $b_{0i}$ and random slope stem $b_{1i}$ from a bivariate normal distribution.\n\n$$Y \\sim N(\\mu, \\sigma^2_\\varepsilon)$${#eq-original}\n$$\\mu = \\beta_0 + \\beta_1 X + b_{0i} + b_{1i} X$$\n$$b \\sim N(0, \\Sigma^2)$$\n\nThe number of groups and the number of observations per group are two important things to check before running a mixed model.\n@fig-hist-id indicates that there are plenty of groups but a large number of groups have only one or just a few observations.\nThis is often problematic in combination with a random slope.\nLet's see what the random slope actually does by cutting some corners and simplifying the mixed model into a set of hierarchical linear models.\nWe have one linear model 'FE' that fits the response using only the fixed effects.\nFor each group we fit another linear model on the _residuals_ of model 'FE'.\nSo when there are only two observations in a group, the random slope model fits a straight line through only two points...\nTo make things even worse, many groups have quite a small span (@fig-id-span).\nImage the worst case were a group has only two observations, both have extreme and opposite residuals from model 'FE' and their span is small.\nThe result will be an extreme random slope...\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndataset %>%\n  count(ID) %>%\n  ggplot(aes(x = n)) + geom_histogram(binwidth = 1)\n```\n\n::: {.cell-output-display}\n![Histogram of the number of observations per group](highly-correlated_files/figure-html/fig-hist-id-1.svg){#fig-hist-id width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndataset %>% \n  group_by(ID) %>% \n  summarise(\n    span = max(X) - min(X),\n    n = n()\n  ) %>%\n  filter(n > 1) %>%\n  ggplot(aes(x = span)) + geom_density()\n```\n\n::: {.cell-output-display}\n![Density of the span (difference between min and max) for all groups with at least 2 observations](highly-correlated_files/figure-html/fig-id-span-1.svg){#fig-id-span width=672}\n:::\n:::\n\n\n# Original model\n\nFirst we fit the original model.\nNotice the perfect negative correlation between the random intercept and the random slope.\nThis triggered, rightly, an alarm with the researcher.\nThe perfect correlation is clear when looking at a scatter plot of the random intercepts and random slopes (@fig-scatter-is).\n@fig-extreme-slopes show the nine most extreme random slopes.\nThe Y-axis displays the difference between the observed $Y$ and the model fit using only the fixed effects ($\\beta_0 + \\beta_1X$).\nNote that the random slopes are not as strong as what we would expect from the naive hierarchical model we described above.\nMixed models apply shrinkage to the coefficients of the random effects, making them less extreme.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- lmer(Y ~ X + (X|ID), data = dataset)\nsummary(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear mixed model fit by REML ['lmerMod']\nFormula: Y ~ X + (X | ID)\n   Data: dataset\n\nREML criterion at convergence: 10753.1\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-8.6576 -0.3511  0.0692  0.4535  6.5774 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr \n ID       (Intercept) 21.050   4.588         \n          X           11.102   3.332    -1.00\n Residual              6.124   2.475         \nNumber of obs: 2234, groups:  ID, 601\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept) -64.3591     0.4219  -152.6\nX            98.0603     0.2783   352.4\n\nCorrelation of Fixed Effects:\n  (Intr)\nX -0.986\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nrf <- ranef(model)$ID %>%\n  select(RandomIntercept = 1, RandomSlope = 2) %>%\n  rownames_to_column(\"ID\") %>%\n  mutate(ID = as.integer(ID))\nggplot(rf, aes(x = RandomIntercept, y = RandomSlope)) + geom_point()\n```\n\n::: {.cell-output-display}\n![Scatterplot of the random intercepts and random slopes.](highly-correlated_files/figure-html/fig-scatter-is-1.png){#fig-scatter-is width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndataset <- dataset %>%\n  mutate(\n    Resid = resid(model),\n    Fixed = predict(model, re.form = ~0)\n  )\nrf %>%\n  arrange(desc(abs(RandomSlope))) %>%\n  slice(1:9) %>%\n  inner_join(dataset, by = \"ID\") %>%\n  ggplot(aes(x = X, y = Y - Fixed)) + \n  geom_point() + \n  geom_hline(yintercept = 0, linetype = 2) + \n  geom_abline(aes(intercept = RandomIntercept, slope = RandomSlope)) +\n  facet_wrap(~ID)\n```\n\n::: {.cell-output-display}\n![Illustration of the most extreme random slopes.](highly-correlated_files/figure-html/fig-extreme-slopes-1.svg){#fig-extreme-slopes width=672}\n:::\n:::\n\n\nUntil now, we focused mainly on the random effects.\nAnother thing one must check are the residuals.\nThe QQ-plot (@fig-qq) indicates that several observations have quite strong residuals.\nThose should be checked by an expert with domain knowledge on the data.\nI recommend to start by looking at the top 20 observations with the most extreme residuals.\nQuestion the data for these observations: e.g. was the measurement correct, was the data entry correct, ...\nWhen the data turns out to be OK, question it's relevance for the model (e.g. is the observation a special case) and question the model itself (e.g. are missing something important in the model, does the model makes sense).\nRefit the model after the data cleaning and repeat the process until you are happy with both the model and the data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(dataset, aes(sample = Resid)) + stat_qq()\n```\n\n::: {.cell-output-display}\n![Density of the residuals](highly-correlated_files/figure-html/fig-qq-1.png){#fig-qq width=672}\n:::\n:::\n\n\n# Potential solutions\n\n## Removing questionable observations\n\nHere we demonstrate what happens in case all observations with strong residuals turn out to be questionable and are removed from the data.\nNote that we **do not** recommend to simply remove all observation with strong residuals.\nInstead have a domain expert scrutinise each observation and remove only those observations which are plain wrong or not relevant.\nThis is something I can't do in this case because I don't have the required domain knowledge.\nFor demonstration purposes I've removed all observations who's residuals are outside the (0.5%, 99.5%) quantile of the theoretical distribution of the residuals.\nThe QQ-plot (fig. \\@ref(fig:qq-cleaned)) now looks OK, but we still have perfect correlation among the random effects.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndataset_cleaned <- dataset %>%\n  filter(abs(Resid) < qnorm(0.995, mean = 0, sd = sigma(model)))\nmodel_cleaned <- lmer(Y ~ X + (X|ID), data = dataset_cleaned)\nsummary(model_cleaned)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear mixed model fit by REML ['lmerMod']\nFormula: Y ~ X + (X | ID)\n   Data: dataset_cleaned\n\nREML criterion at convergence: 9182.5\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.6053 -0.4946  0.0577  0.5772  3.4444 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr \n ID       (Intercept) 20.973   4.580         \n          X            9.500   3.082    -1.00\n Residual              3.241   1.800         \nNumber of obs: 2188, groups:  ID, 598\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept) -64.6873     0.3471  -186.3\nX            98.3516     0.2257   435.7\n\nCorrelation of Fixed Effects:\n  (Intr)\nX -0.989\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndataset_cleaned %>% mutate(Resid = resid(model_cleaned)) %>%\n  ggplot(aes(sample = Resid)) + stat_qq()\n```\n\n::: {.cell-output-display}\n![QQ-plot for the original model on the cleaned dataset.](highly-correlated_files/figure-html/fig-qq-cleaned-1.png){#fig-qq-cleaned width=672}\n:::\n:::\n\n\n## Centring and scaling\n\nAnother thing that often can help is centring and scaling the data.\nIn this case we centre to a zero mean and scale to a standard deviation of 1.\nPersonally I prefer to centre to some meaningful value in the data.\nE.g. when the variable is the year of the observation I would centre to the first year, last year or some other important year within the dataset.\nThis makes the interpretation of the model parameters easier.\nI usually scale variables by some power of 10 again because of the interpretation of the parameters.\n\nWe will keep using the cleaned dataset so that the problematic observation don't interfere with the effect of centring and scaling.\nScaling improves the correlation between the random intercept and the random slope.\nIt is no longer a perfect correlation but still quite strong (@fig-scatter-is-centred).\nNote that the sign of the correlation has changed.\nAlthough we removed the questionable observations, there are still some groups of observations with quite strong deviations from the fixed effects part from the model (@fig-extreme-slopes-centred).\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndataset_cleaned <- dataset_cleaned %>%\n  mutate(\n    Xcs = scale(X, center = TRUE, scale = TRUE)\n  )\nmodel_centered <- lmer(Y ~ X + (Xcs | ID), data = dataset_cleaned)\nsummary(model_centered)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear mixed model fit by REML ['lmerMod']\nFormula: Y ~ X + (Xcs | ID)\n   Data: dataset_cleaned\n\nREML criterion at convergence: 9182.5\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.6053 -0.4946  0.0577  0.5772  3.4444 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr\n ID       (Intercept) 0.3996   0.6322       \n          Xcs         1.4161   1.1900   0.88\n Residual             3.2413   1.8004       \nNumber of obs: 2188, groups:  ID, 598\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept) -64.6873     0.3471  -186.4\nX            98.3516     0.2257   435.7\n\nCorrelation of Fixed Effects:\n  (Intr)\nX -0.989\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nrf <- ranef(model_centered)$ID %>%\n  select(RandomIntercept = 1, RandomSlope = 2) %>%\n  rownames_to_column(\"ID\") %>%\n  mutate(ID = as.integer(ID))\nggplot(rf, aes(x = RandomIntercept, y = RandomSlope)) + geom_point()\n```\n\n::: {.cell-output-display}\n![Scatterplot of the random intercepts and random slopes after centering and scaling.](highly-correlated_files/figure-html/fig-scatter-is-centred-1.png){#fig-scatter-is-centred width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndataset_cleaned <- dataset_cleaned %>%\n  mutate(\n    Resid = resid(model_centered),\n    Fixed = predict(model_centered, re.form = ~0)\n  )\nrf %>%\n  arrange(desc(abs(RandomSlope))) %>%\n  slice(1:9) %>%\n  inner_join(dataset_cleaned, by = \"ID\") %>%\n  ggplot(aes(x = X, y = Y - Fixed)) + \n  geom_point() + \n  geom_hline(yintercept = 0, linetype = 2) + \n  geom_abline(aes(intercept = RandomIntercept, slope = RandomSlope)) +\n  facet_wrap(~ID)\n```\n\n::: {.cell-output-display}\n![Illustration of the most extreme random slopes after centering and scaling.](highly-correlated_files/figure-html/fig-extreme-slopes-centred-1.svg){#fig-extreme-slopes-centred width=672}\n:::\n:::\n\n\n## Simplifying the model\n\nBased on @fig-hist-id and @fig-id-span we already concluded that a random slope might be pushing it for this data set.\nSo an obvious solution is to remove the random slope and only keep the random intercept.\nThough there still are quite a large number of groups with only one observation (@fig-hist-id), this is often less problematic in case you have plenty of groups with multiple observations.\nNote that the variance of the random intercept of this model is much smaller than in the previous models.\nThe random intercept model is not as good as the random slope model in terms of AIC, but this comparison is a bit pointless since the random slope model is not trustworthy.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_simple <- lmer(Y ~ X + (1|ID), data = dataset_cleaned)\nsummary(model_simple)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear mixed model fit by REML ['lmerMod']\nFormula: Y ~ X + (1 | ID)\n   Data: dataset_cleaned\n\nREML criterion at convergence: 9593.6\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-6.2261 -0.4503  0.0682  0.5238  6.0184 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n ID       (Intercept) 1.897    1.377   \n Residual             3.688    1.921   \nNumber of obs: 2188, groups:  ID, 598\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept) -64.1588     0.2736  -234.5\nX            97.9788     0.1572   623.2\n\nCorrelation of Fixed Effects:\n  (Intr)\nX -0.959\n```\n:::\n\n```{.r .cell-code}\nanova(model_centered, model_simple)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nrefitting model(s) with ML (instead of REML)\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nData: dataset_cleaned\nModels:\nmodel_simple: Y ~ X + (1 | ID)\nmodel_centered: Y ~ X + (Xcs | ID)\n               npar    AIC    BIC  logLik deviance  Chisq Df Pr(>Chisq)    \nmodel_simple      4 9596.4 9619.2 -4794.2   9588.4                         \nmodel_centered    6 9189.2 9223.3 -4588.6   9177.2 411.22  2  < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\n## Transformations\n\n@fig-density indicated that the distribution of both $X$ and $Y$ is quite skewed.\nA $log$-transformation reduces the skewness (@fig-log-density) and reveals a quadratic relation between $\\log(X)$ and $\\log(Y)$ (@fig-scatter-log).\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndataset_cleaned %>%\n  select(X, Y) %>%\n  gather(\"Variable\", \"Value\") %>%\n  ggplot(aes(x = log(Value))) + geom_density() + facet_wrap(~Variable, scales = \"free\")\n```\n\n::: {.cell-output-display}\n![Density of $X$ and $Y$ after log-transformation](highly-correlated_files/figure-html/fig-log-density-1.svg){#fig-log-density width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(dataset_cleaned, aes(x = X, y = Y)) + geom_point(alpha = 0.1) + \n  coord_trans(x = \"log\", y = \"log\")\n```\n\n::: {.cell-output-display}\n![Scatterplot after log-transformation.](highly-correlated_files/figure-html/fig-scatter-log-1.png){#fig-scatter-log width=672}\n:::\n:::\n\n\nThis might be a relevant transformation, but it needs to be checked by a domain expert because this random slope model @eq-log expresses a different relation between $X$ and $Y$.\nThe fixed part of model @eq-log becomes $Y \\sim e^{\\gamma_0 }X^{\\gamma_1}$ after back transformation.\n\n$$\\log(Y) \\sim N(\\eta, \\sigma^2_\\varepsilon)$${#eq-log}\n$$\\eta = \\gamma_0 + \\gamma_1 \\log(X) + c_{0i}$$\n$$c \\sim N(0, \\sigma^2)$$\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndataset_cleaned <- dataset_cleaned %>% \n  mutate(\n    logY = log(Y), \n    logX = log(X)\n)\nmodel_log <- lmer(logY ~ logX + (1|ID), data = dataset_cleaned)\nsummary(model_log)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear mixed model fit by REML ['lmerMod']\nFormula: logY ~ logX + (1 | ID)\n   Data: dataset_cleaned\n\nREML criterion at convergence: -8723.6\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-6.3306 -0.4654  0.1075  0.5517  2.8275 \n\nRandom effects:\n Groups   Name        Variance  Std.Dev.\n ID       (Intercept) 0.0006140 0.02478 \n Residual             0.0007947 0.02819 \nNumber of obs: 2188, groups:  ID, 598\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept) 3.747007   0.002623  1428.6\nlogX        1.612781   0.004681   344.5\n\nCorrelation of Fixed Effects:\n     (Intr)\nlogX -0.871\n```\n:::\n:::\n\n\nA quadratic fixed effect of $\\log(X)$ improves the model a lot.\nThe resulting fit is given in @fig-fit-log.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_log2 <- lmer(logY ~ poly(logX, 2) + (1|ID), data = dataset_cleaned)\nanova(model_log, model_log2)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nrefitting model(s) with ML (instead of REML)\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nData: dataset_cleaned\nModels:\nmodel_log: logY ~ logX + (1 | ID)\nmodel_log2: logY ~ poly(logX, 2) + (1 | ID)\n           npar    AIC      BIC logLik deviance  Chisq Df Pr(>Chisq)    \nmodel_log     4  -8736  -8713.2 4372.0    -8744                         \nmodel_log2    5 -10434 -10406.0 5222.2   -10444 1700.4  1  < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n\n```{.r .cell-code}\nsummary(model_log2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear mixed model fit by REML ['lmerMod']\nFormula: logY ~ poly(logX, 2) + (1 | ID)\n   Data: dataset_cleaned\n\nREML criterion at convergence: -10420.7\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-4.6171 -0.4938  0.0662  0.5359  4.1296 \n\nRandom effects:\n Groups   Name        Variance  Std.Dev.\n ID       (Intercept) 9.311e-05 0.00965 \n Residual             4.329e-04 0.02081 \nNumber of obs: 2188, groups:  ID, 598\n\nFixed effects:\n                 Estimate Std. Error t value\n(Intercept)     4.5311687  0.0006646 6817.57\npoly(logX, 2)1 15.8221541  0.0273352  578.82\npoly(logX, 2)2 -1.3249513  0.0250247  -52.95\n\nCorrelation of Fixed Effects:\n            (Intr) p(X,2)1\nply(lgX,2)1 -0.007        \nply(lgX,2)2 -0.029 -0.065 \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndataset_cleaned <- dataset_cleaned %>%\n  mutate(Fixed = predict(model_log2, re.form = ~0))\nggplot(dataset_cleaned, aes(x = logX, y = logY)) +\n  geom_point(alpha = 0.1) +\n  geom_line(aes(y = Fixed), colour = \"blue\")\n```\n\n::: {.cell-output-display}\n![Predictions for the fixed effect of the quadratic model on $\\log(X)$](highly-correlated_files/figure-html/fig-fit-log-1.png){#fig-fit-log width=672}\n:::\n:::\n\n\n# Conclusions\n\n- First of all the data set needs to be checked for potential errors in the data.\n- The current design of the data does not support a random slope model.\n- A transformation of the variables might be relevant.\n\n## Session info\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsessioninfo::session_info()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.1 (2023-06-16)\n os       Ubuntu 22.04.3 LTS\n system   x86_64, linux-gnu\n ui       X11\n language nl_BE:nl\n collate  nl_BE.UTF-8\n ctype    nl_BE.UTF-8\n tz       Europe/Brussels\n date     2023-08-30\n pandoc   3.1.1 @ /usr/lib/rstudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package     * version    date (UTC) lib source\n bit           4.0.5      2022-11-15 [1] CRAN (R 4.3.0)\n bit64         4.0.5      2020-08-30 [1] CRAN (R 4.3.0)\n boot          1.3-28.1   2022-11-22 [1] CRAN (R 4.3.0)\n cli           3.6.1      2023-03-23 [1] CRAN (R 4.3.0)\n colorspace    2.1-0      2023-01-23 [1] CRAN (R 4.3.0)\n crayon        1.5.2      2022-09-29 [1] CRAN (R 4.3.0)\n digest        0.6.32     2023-06-26 [1] CRAN (R 4.3.1)\n dplyr       * 1.1.2      2023-04-20 [1] CRAN (R 4.3.0)\n evaluate      0.21       2023-05-05 [1] CRAN (R 4.3.0)\n fansi         1.0.4      2023-01-22 [1] CRAN (R 4.3.0)\n farver        2.1.1      2022-07-06 [1] CRAN (R 4.3.0)\n fastmap       1.1.1      2023-02-24 [1] CRAN (R 4.3.0)\n forcats     * 1.0.0      2023-01-29 [1] CRAN (R 4.3.0)\n generics      0.1.3      2022-07-05 [1] CRAN (R 4.3.0)\n ggplot2     * 3.4.2      2023-04-03 [1] CRAN (R 4.3.0)\n glue          1.6.2      2022-02-24 [1] CRAN (R 4.3.0)\n gtable        0.3.3      2023-03-21 [1] CRAN (R 4.3.0)\n hms           1.1.3      2023-03-21 [1] CRAN (R 4.3.0)\n htmltools     0.5.5      2023-03-23 [1] CRAN (R 4.3.0)\n htmlwidgets   1.6.2      2023-03-17 [1] CRAN (R 4.3.0)\n jsonlite      1.8.7      2023-06-29 [1] CRAN (R 4.3.1)\n knitr         1.43       2023-05-25 [1] CRAN (R 4.3.0)\n labeling      0.4.2      2020-10-20 [1] CRAN (R 4.3.0)\n lattice       0.21-8     2023-04-05 [4] CRAN (R 4.3.0)\n lifecycle     1.0.3      2022-10-07 [1] CRAN (R 4.3.0)\n lme4        * 1.1-34     2023-07-04 [1] CRAN (R 4.3.1)\n lubridate   * 1.9.2.9000 2023-05-15 [1] https://inbo.r-universe.dev (R 4.3.0)\n magrittr      2.0.3      2022-03-30 [1] CRAN (R 4.3.0)\n MASS          7.3-60     2023-05-04 [4] CRAN (R 4.3.1)\n Matrix      * 1.5-4.1    2023-05-18 [1] CRAN (R 4.3.0)\n minqa         1.2.5      2022-10-19 [1] CRAN (R 4.3.0)\n munsell       0.5.0      2018-06-12 [1] CRAN (R 4.3.0)\n nlme          3.1-162    2023-01-31 [1] CRAN (R 4.3.0)\n nloptr        2.0.3      2022-05-26 [1] CRAN (R 4.3.0)\n pillar        1.9.0      2023-03-22 [1] CRAN (R 4.3.0)\n pkgconfig     2.0.3      2019-09-22 [1] CRAN (R 4.3.0)\n purrr       * 1.0.1      2023-01-10 [1] CRAN (R 4.3.0)\n R6            2.5.1      2021-08-19 [1] CRAN (R 4.3.0)\n Rcpp          1.0.10     2023-01-22 [1] CRAN (R 4.3.0)\n readr       * 2.1.4      2023-02-10 [1] CRAN (R 4.3.0)\n rlang         1.1.1      2023-04-28 [1] CRAN (R 4.3.0)\n rmarkdown     2.23       2023-07-01 [1] CRAN (R 4.3.1)\n rstudioapi    0.14       2022-08-22 [1] CRAN (R 4.3.0)\n scales        1.2.1      2022-08-20 [1] CRAN (R 4.3.0)\n sessioninfo   1.2.2      2021-12-06 [1] CRAN (R 4.3.0)\n stringi       1.7.12     2023-01-11 [1] CRAN (R 4.3.0)\n stringr     * 1.5.0      2022-12-02 [1] CRAN (R 4.3.0)\n tibble      * 3.2.1      2023-03-20 [1] CRAN (R 4.3.0)\n tidyr       * 1.3.0      2023-01-24 [1] CRAN (R 4.3.0)\n tidyselect    1.2.0      2022-10-10 [1] CRAN (R 4.3.0)\n tidyverse   * 2.0.0      2023-02-22 [1] CRAN (R 4.3.0)\n timechange    0.2.0      2023-01-11 [1] CRAN (R 4.3.0)\n tzdb          0.4.0      2023-05-12 [1] CRAN (R 4.3.0)\n utf8          1.2.3      2023-01-31 [1] CRAN (R 4.3.0)\n vctrs         0.6.3      2023-06-14 [1] CRAN (R 4.3.0)\n vroom         1.6.3      2023-04-28 [1] CRAN (R 4.3.0)\n withr         2.5.0      2022-03-03 [1] CRAN (R 4.3.0)\n xfun          0.39       2023-04-20 [1] CRAN (R 4.3.0)\n yaml          2.3.7      2023-01-23 [1] CRAN (R 4.3.0)\n\n [1] /home/thierry/R/x86_64-pc-linux-gnu-library/4.3\n [2] /usr/local/lib/R/site-library\n [3] /usr/lib/R/site-library\n [4] /usr/lib/R/library\n\n──────────────────────────────────────────────────────────────────────────────\n```\n:::\n:::\n",
    "supporting": [
      "highly-correlated_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}